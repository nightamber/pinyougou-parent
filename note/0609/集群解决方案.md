# 集群解决方案

## 课程目标

- 目标 1：说出什么是集群以及与分布式的区别
- 目标 2：能够搭建 Zookeeper 集群
- 目标 3：能够搭建 SolrCloud 集群
- 目标 4：能够搭建 RedisCluster 集群

## 集群概述

### 什么是集群

#### 集群概念

集群是一种计算机系统， 它通过一组松散集成的计算机软件和/或硬件连接起来高度紧密地协作完成计算工作。在某种意义上，他们可以被看作是一台计算机。集群系统中的单个计算机通常称为节点，通常通过局域网连接，但也有其它的可能连接方式。集群计算机通常用来改进单个计算机的计算速度和/或可靠性。一般情况下集群计算机比单个计算机，比如工作站或超级计算机性能价格比要高得多。

![52850616843](H:\itheima大数据项目班\项目班\0609\iamges\1528506168437.png)





#### 集群的特点

集群拥有以下两个特点：

1. 可扩展性：集群的性能不限制于单一的服务实体，新的服务实体可以动态的添加到集群，从而增强集群的性能。

2. 高可用性：集群当其中一个节点发生故障时，这台节点上面所运行的应用程序将在另一台节点被自动接管，消除单点故障对于增强数据可用性、可达性和可靠性是非常重要的。

  ​

#### 集群的两大能力



集群必须拥有以下两大能力：

1. 负载均衡：负载均衡把任务比较均匀的分布到集群环境下的计算和网络资源，以提高数据吞吐量。
2. 错误恢复：如果集群中的某一台服务器由于故障或者维护需要无法使用，资源和应用程序将转移到可用的集群节点上。这种由于某个节点的资源不能工作，另一个可用节点中的资源能够透明的接管并继续完成任务的过程，叫做错误恢复。
3. 负载均衡和错误恢复要求各服务实体中有执行同一任务的资源存在，而且对于同一任务的各个资源来说，执行任务所需的信息视图必须是相同的

### 集群与分布式的区别

说到集群，可能大家会立刻联想到另一个和它很相近的一个词----“分布式”。那么集群和分布式是一回事吗？有什么联系和区别呢?
相同点：
分布式和集群都是需要有很多节点服务器通过网络协同工作完成整体的任务目标。
不同点：
分布式是指将业务系统进行拆分，即分布式的每一个节点都是实现不同的功能。而集群每个节点做的是同一件事情。

如下图，每个人都有不同的分工，一起协作干一件事，叫做“分布式”

![52850630493](H:\itheima大数据项目班\项目班\0609\iamges\1528506304932.png)



再看下图：每个划桨人干的都是一样的活，叫做集群。



![52850632377](H:\itheima大数据项目班\项目班\0609\iamges\1528506323771.png)





分布式的每一个节点也可以做成集群。其实这个赛龙舟的图，总整体来看属于分布式,包括打鼓和划桨两个分布式节点，而划桨的节点又是集群的形态。
现实生活中例子还有很多，例如，这样古代乐队的图就属于集群

![52850635521](H:\itheima大数据项目班\项目班\0609\iamges\1528506355214.png)

而现代乐队这样图就是分布式啦

![52850636977](H:\itheima大数据项目班\项目班\0609\iamges\1528506369774.png)



## Zookeeper  集群

### Zookeeper  集群简介

#### 为什么搭建 Zookeeper  集群

大部分分布式应用需要一个主控、协调器或者控制器来管理物理分布的子进程。目前，大多数都要开发私有的协调程序，缺乏一个通用机制，协调程序的反复编写浪费，且难以形成通用、伸缩性好的协调器，zookeeper 提供通用的分布式锁服务，用以协调分布式应用。所以说 zookeeper 是分布式应用的协作服务。

zookeeper 作为注册中心，服务器和客户端都要访问，如果有大量的并发，肯定会有等待。所以可以通过 zookeeper 集群解决。

下面是 zookeeper 集群部署结构图：

![52850642798](H:\itheima大数据项目班\项目班\0609\iamges\1528506427981.png)



#### 了解 Leader 选举

Zookeeper 的启动过程中 leader 选举是非常重要而且最复杂的一个环节。那么什么是leader选举呢？zookeeper为什么需要leader选举呢？zookeeper的leader选举的过程又是什么样子的？

首先我们来看看什么是 leader 选举。其实这个很好理解，leader 选举就像总统选举一样，每人一票，获得多数票的人就当选为总统了。在 zookeeper 集群中也是一样，每个节点都会投票，如果某个节点获得超过半数以上的节点的投票，则该节点就是 leader 节点了。



以一个简单的例子来说明整个选举的过程.



假设有五台服务器组成的 zookeeper 集群,它们的 id 从 1-5,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动,来看看会发生什么 。

1) 服务器 1 启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的
选举状态一直是 LOOKING 状态
2) 服务器 2 启动,它与最开始启动的服务器 1 进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是 3),所以服务器 1,2 还是继续保持 LOOKING 状态.
3) 服务器 3 启动,根据前面的理论分析,服务器 3 成为服务器 1,2,3 中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的 leader.
4) 服务器 4 启动,根据前面的分析,理论上服务器 4 应该是服务器 1,2,3,4 中最大的,但是由于前面已经有半数以上的服务器选举了服务器 3,所以它只能接收当小弟的命了.

5) 服务器 5 启动,同 4 一样,当小弟

###   搭建 Zookeeper  集群

####  搭建要求

真实的集群是需要部署在不同的服务器上的，但是在我们测试时同时启动十几个虚拟机内存会吃不消，所以我们通常会搭建 伪集群，也就是把所有的服务都搭建在一台虚拟机上，用端口进行区分。

我们这里要求搭建一个三个节点的 Zookeeper 集群（伪集群）。



####   准备工作

重新部署一台虚拟机作为我们搭建集群的测试服务器。
（1）安装 JDK 【此步骤省略】。
（2）Zookeeper 压缩包上传到服务器
（3）将 Zookeeper 解压 ，创建 data 目录 ，将 conf 下 zoo_sample.cfg 文件改名为 zoo.cfg
（4）建立/usr/local/zookeeper-cluster 目录，将解压后的 Zookeeper 复制到以下三个目录

/usr/local/zookeeper-cluster/zookeeper-1
/usr/local/zookeeper-cluster/zookeeper-2
/usr/local/zookeeper-cluster/zookeeper-3



```shell
[root@localhost ~]# mkdir /usr/local/zookeeper-cluster
[root@localhost ~]# cp -r zookeeper-3.4.6 /usr/local/zookeeper-cluster/zookeeper-1
[root@localhost ~]# cp -r zookeeper-3.4.6 /usr/local/zookeeper-cluster/zookeeper-2
[root@localhost ~]# cp -r zookeeper-3.4.6 /usr/local/zookeeper-cluster/zookeeper-3
```

（5） 配置每一个 Zookeeper 的 dataDir（zoo.cfg） clientPort 分别为 2181 2182 2183修改/usr/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfg

```cfg
clientPort=2181
dataDir=/usr/local/zookeeper-cluster/zookeeper-1/data
```

修改/usr/local/zookeeper-cluster/zookeeper-2/conf/zoo.cfg

```cfg
clientPort=2182
dataDir=/usr/local/zookeeper-cluster/zookeeper-2/data
```

修改/usr/local/zookeeper-cluster/zookeeper-3/conf/zoo.cfg

```cfg
clientPort=2183
dataDir=/usr/local/zookeeper-cluster/zookeeper-3/data
```

#### 配置集群

（1）在每个 zookeeper 的 data 目录下创建一个 myid 文件，内容分别是 1、2、3 。这个文件就是记录每个服务器的 ID

------- 知识点小贴士------
如果你要创建的文本文件内容比较简单，我们可以通过 echo 命令快速创建文件
格式为：
echo 内容 >文件名
例如我们为第一个 zookeeper 指定 ID 为 1，则输入命令

![52850669788](H:\itheima大数据项目班\项目班\0609\iamges\1528506697883.png)



（2）在每一个 zookeeper 的 zoo.cfg 配置客户端访问端口（clientPort）和集群服务器 IP 列表。

集群服务器 IP 列表如下

```cfg
server.1=192.168.25.140:2881:3881
server.2=192.168.25.140:2882:3882
server.3=192.168.25.140:2883:3883
```

解释：server.服务器 ID=服务器 IP 地址：服务器之间通信端口：服务器之间投票选举端口

----- 知识点小贴士-----
我们可以使用 EditPlus 远程修改服务器的文本文件的内容，更加便捷
（1）在菜单选择 FTP Settings

![52850676168](H:\itheima大数据项目班\项目班\0609\iamges\1528506761683.png)

（2）点击 ADD 按钮

![52850677476](H:\itheima大数据项目班\项目班\0609\iamges\1528506774764.png)



（3）输入服务器信息

![52850678899](H:\itheima大数据项目班\项目班\0609\iamges\1528506788999.png)

（4）点击高级选项按钮

![52850680465](H:\itheima大数据项目班\项目班\0609\iamges\1528506804650.png)

（5）选择 SFTP 端口 22

![52850681801](H:\itheima大数据项目班\项目班\0609\iamges\1528506818013.png)



（6）OK 。完成配置

![52850684127](H:\itheima大数据项目班\项目班\0609\iamges\1528506841271.png)



连接

![52850685574](H:\itheima大数据项目班\项目班\0609\iamges\1528506855748.png)



![52850686541](H:\itheima大数据项目班\项目班\0609\iamges\1528506865411.png)



![52850687689](H:\itheima大数据项目班\项目班\0609\iamges\1528506876894.png)

哈哈，无敌啦~~~~ 你可能要问，老师，你为啥不早告诉我有这一招 ！

#### 启动集群

启动集群就是分别启动每个实例。

![52850693445](H:\itheima大数据项目班\项目班\0609\iamges\1528506934451.png)



启动后我们查询一下每个实例的运行状态
先查询第一个服务

![52850695171](H:\itheima大数据项目班\项目班\0609\iamges\1528506951718.png)

Mode 为 follower 表示是 跟随者（从）
再查询第二个服务 Mod 为 leader 表示是 领导者（主）

![52850696444](H:\itheima大数据项目班\项目班\0609\iamges\1528506964440.png)

查询第三个为跟随者（从）

![52850697727](H:\itheima大数据项目班\项目班\0609\iamges\1528506977271.png)

#### 模拟集群异常

（1）首先我们先测试如果是从服务器挂掉，会怎么样
把 3 号服务器停掉，观察 1 号和 2 号，发现状态并没有变化

![52850701988](H:\itheima大数据项目班\项目班\0609\iamges\1528507019882.png)

由此得出结论，3 个节点的集群，从服务器挂掉，集群正常

（2）我们再把 1 号服务器（从服务器）也停掉，查看 2 号（主服务器）的状态，发现已经停止运行了。

![52850703414](H:\itheima大数据项目班\项目班\0609\iamges\1528507034145.png)

由此得出结论，3 个节点的集群，2 个从服务器都挂掉，主服务器也无法运行。因为可运行的机器没有超过集群总数量的半数。

（3）我们再次把 1 号服务器启动起来，发现 2 号服务器又开始正常工作了。而且依然是领导者。

![52850708429](H:\itheima大数据项目班\项目班\0609\iamges\1528507084299.png)

发现新的 leader 产生了~
由此我们得出结论，当集群中的主服务器挂了，集群中的其他服务器会自动进行选举状态，然后产生新得 leader

（5）我们再次测试，当我们把 2 号服务器重新启动起来（汗~~这是诈尸啊!）启动后，会发生什么？2 号服务器会再次成为新的领导吗？我们看结果

![52850711641](H:\itheima大数据项目班\项目班\0609\iamges\1528507116412.png)

![52850712479](H:\itheima大数据项目班\项目班\0609\iamges\1528507124796.png)

我们会发现，2 号服务器启动后依然是跟随者（从服务器），3 号服务器依然是领导者（主服务器），没有撼动 3 号服务器的领导地位。哎~退休了就是退休了，说了不算了，哈哈。由此我们得出结论，当领导者产生后，再次有新服务器加入集群，不会影响到现任领导者。

####  Dubbox  连接 zookeeper 

修改服务提供者和服务调用者的 spring 配置文件

```xml
<!-- 指定注册中心地址 -->
<dubbo:registry
protocol="zookeeper"
address="192.168.25.140:2181,192.168.25.140:2182,192.168.25.140:2183">
</dubbo:registry>
```

## SolrCloud



### SolrCloud  简介

#### 什么是 SolrCloud

SolrCloud(solr 云)是 Solr 提供的分布式搜索方案，当你需要大规模，容错，分布式索引和检索能力时使用 SolrCloud。当一个系统的索引数据量少的时候是不需要使用 SolrCloud的，当索引量很大，搜索请求并发很高，这时需要使用 SolrCloud 来满足这些需求

SolrCloud 是基于 Solr 和 Zookeeper 的分布式搜索方案，它的主要思想是使用Zookeeper 作为集群的配置信息中心。

它有几个特色功能：
1）集中式的配置信息
2）自动容错
3）近实时搜索
4）查询时自动负载均衡



#### SolrCloud 系统框架

![52850730437](H:\itheima大数据项目班\项目班\0609\iamges\1528507304374.png)

【1】物理结构
三个 Solr 实例（ 每个实例包括两个 Core），组成一个 SolrCloud。
【2】逻辑结构
索引集合包括两个 Shard（shard1 和 shard2），shard1 和 shard2 分别由三个 Core组成，其中一个 Leader 两个 Replication，Leader 是由 zookeeper 选举产生，zookeeper 控制每个 shard 上三个 Core 的索引数据一致，解决高可用问题。用户发起索引请求分别从 shard1 和 shard2 上获取，解决高并发问题。

（1）Collection
Collection 在 SolrCloud 集群中是一个逻辑意义上的完整的索引结构。它常常被划分为一个或多个 Shard（分片），它们使用相同的配置信息。
比如：针对商品信息搜索可以创建一个 collection。
collection=shard1+shard2+....+shardX



（ 2 ） Core每个 Core 是 Solr 中一个独立运行单位，提供 索引和搜索服务。一个 shard 需要由一个Core 或多个 Core 组成。由于 collection 由多个 shard 组成所以 collection 一般由多个core 组成。

（ 3 ）Master  或 Slave
Master 是 master-slave 结构中的主结点（通常说主服务器），Slave 是 master-slave 结构中的从结点（通常说从服务器或备服务器）。同一个 Shard 下 master 和 slave 存储的数据是一致的，这是为了达到高可用目的。

（ 4 ）Shard
Collection 的逻辑分片。每个 Shard 被化成一个或者多个 replication，通过选举确定哪个是Leader。

### 搭建 SolrCloud

#### 搭建要求

![52850752430](H:\itheima大数据项目班\项目班\0609\iamges\1528507524306.png)



Zookeeper 作为集群的管理工具
1、集群管理：容错、负载均衡。
2、配置文件的集中管理

3、集群的入口
需要实现 zookeeper 高可用，需要搭建 zookeeper 集群。建议是奇数节点。需要三个zookeeper 服务器。
搭建 solr 集群需要 7 台服务器（搭建伪分布式，建议虚拟机的内存 1G 以上）：
需要三个 zookeeper 节点
需要四个 tomcat 节点。

#### 准备工作

**环境准备**
CentOS-6.5-i386-bin-DVD1.iso
jdk-7u72-linux-i586.tar.gz
apache-tomcat-7.0.47.tar.gz
zookeeper-3.4.6.tar.gz
solr-4.10.3.tgz

**步骤：**
（1）搭建 Zookeeper 集群（我们在上一小节已经完成）
（2）将已经部署完 solr 的 tomcat 的上传到 linux
（3）在 linux 中创建文件夹 /usr/local/solr-cloud 创建 4 个 tomcat 实例

```shell
[root@localhost ~]# mkdir /usr/local/solr-cloud
[root@localhost ~]# cp -r tomcat-solr /usr/local/solr-cloud/tomcat-1
[root@localhost ~]# cp -r tomcat-solr /usr/local/solr-cloud/tomcat-2
[root@localhost ~]# cp -r tomcat-solr /usr/local/solr-cloud/tomcat-3
[root@localhost ~]# cp -r tomcat-solr /usr/local/solr-cloud/tomcat-4
```

（4）将本地的 solrhome 上传到 linux
（5）在 linux 中创建文件夹 /usr/local/solrhomes ,将 solrhome 复制 4 份

```shell
[root@localhost ~]# mkdir /usr/local/solrhomes
[root@localhost ~]# cp -r solrhome /usr/local/solrhomes/solrhome-1
[root@localhost ~]# cp -r solrhome /usr/local/solrhomes/solrhome-2
[root@localhost ~]# cp -r solrhome /usr/local/solrhomes/solrhome-3
[root@localhost ~]# cp -r solrhome /usr/local/solrhomes/solrhome-4
```

（6）修改每个 solr 的 web.xml 文件, 关联 solrhome

```xml
<env-entry>
<env-entry-name>solr/home</env-entry-name>
<env-entry-value>/usr/local/solrhomes/solrhome-1</env-entry-value>
<env-entry-type>java.lang.String</env-entry-type>
</env-entry>
```

（7）修改每个 tomcat 的原运行端口 8085 8080 8009 ，分别为

| 8105 | 8180 | 8109 |
| ---- | ---- | ---- |
| 8205 | 8280 | 8209 |
| 8305 | 8380 | 8309 |
| 8405 | 8480 | 8409 |

------ 知识点小贴士 ------
8005 端口是用来关闭 TOMCAT 服务的端口。
8080 端口，负责建立 HTTP 连接。在通过浏览器访问 Tomcat 服务器的 Web 应用时，使用的就是这个连
接器。
8009 端口，负责和其他的 HTTP 服务器建立连接。在把 Tomcat 与其他 HTTP 服务器集成时，就需要用
到这个连接器。

#### 配置集群

（1）修改每个 tomcat 实例 bin 目录下的 catalina.sh 文件

把此配置添加到 catalina.sh 中( 第 234 行 ) ：

```sh
JAVA_OPTS="-DzkHost=192.168.25.140:2181,192.168.25.140:2182,192.168.25.140:2183"
```

JAVA_OPTS ,顾名思义,是用来设置 JVM 相关运行参数的变量 . 此配置用于在 tomcat 启动时找到 zookeeper 集群

（2）配置 solrCloud 相关的配置。每个 solrhome 下都有一个 solr.xml，把其中的 ip 及端口号配置好（是对应的 tomcat 的 IP 和端口）

solrhomes/solrhome-1/solr.xml

```xml
<solrcloud>
<str name="host">192.168.25.140</str>
<int name="hostPort">8180</int>
<str name="hostContext">${hostContext:solr}</str>
<int name="zkClientTimeout">${zkClientTimeout:30000}</int>
<bool name="genericCoreNodeNames">${genericCoreNodeNames:true}</bool>
</solrcloud>
```

solrhomes/solrhome-2/solr.xml

```xml
<solrcloud>
<str name="host">192.168.25.140</str>
<int name="hostPort">8280</int>
<str name="hostContext">${hostContext:solr}</str>
<int name="zkClientTimeout">${zkClientTimeout:30000}</int>
<bool name="genericCoreNodeNames">${genericCoreNodeNames:true}</bool>
</solrcloud>
```



solrhomes/solrhome-3/solr.xml

```xml
<solrcloud>
<str name="host">192.168.25.140</str>
<int name="hostPort">8380</int>
<str name="hostContext">${hostContext:solr}</str>
<int name="zkClientTimeout">${zkClientTimeout:30000}</int>
<bool name="genericCoreNodeNames">${genericCoreNodeNames:true}</bool>
</solrcloud>
```

solrhomes/solrhome-4/solr.xml

```xml
<solrcloud>
<str name="host">192.168.25.140</str>
<int name="hostPort">8480</int>
<str name="hostContext">${hostContext:solr}</str>
<int name="zkClientTimeout">${zkClientTimeout:30000}</int>
<bool name="genericCoreNodeNames">${genericCoreNodeNames:true}</bool>
</solrcloud>
```

（3）让 zookeeper 统一管理配置文件。需要把 solrhome 下 collection1/conf 目录上传到zookeeper。上传任意 solrhome 中的配置文件即可。
我们需要使用 solr 给我们提供的工具上传配置文件：

solr-4.10.3/example/scripts/cloud-scripts/zkcli.sh

将 solr-4.10.3 压缩包上传到 linux，解压，然后进入 solr-4.10.3/example/scripts/cloud-scripts目录 ，执行下列命令

```shell
./zkcli.sh -zkhost 192.168.25.140:2181,192.168.25.140:2182,192.168.25.140:2183 -cmd upconfig -confdir /usr/local/solrhomes/solrhome-1/collection1/conf -confname myconf
```

参数解释
-zkhost ：指定 zookeeper 地址列表
-cmd ：指定命令。upconfig 为上传配置的命令
-confdir : 配置文件所在目录
-confname : 配置名称

#### 启动集群

（1）启动每个 tomcat 实例。要保证 zookeeper 集群是启动状态。

| ---- 知识点小贴士 -----                            |
| -------------------------------------------------- |
| 如果你想让某个文件夹下都可以执行，使用以下命令实现 |

```shell
chmod -R 777 solr-cloud
```

（2）访问集群
地址栏输入 http://192.168.25.140:8180/solr ，可以看到 Solr 集群版的界面

![52850801535](H:\itheima大数据项目班\项目班\0609\iamges\1528508015359.png)



下图表示的是，一个主节点 ，三个从节点。

![52850803605](H:\itheima大数据项目班\项目班\0609\iamges\1528508036054.png)

### SpringDataSolr  连接 SolrCloud

在 SolrJ 中提供一个叫做 CloudSolrServer 的类，它是 SolrServer 的子类，用于连接 solrCloud它的构造参数就是 zookeeper 的地址列表，另外它要求要指定 defaultCollection 属性（默认的 collection 名称）

我们现在修改 springDataSolrDemo 工程的配置文件 ，把原来的 solr-server 注销，替换为CloudSolrServer .指定构造参数为地址列表，设置默认 collection 名称

```xml
<!-- solr 服务器地址
<solr:solr-server id="solrServer" url="http://192.168.25.129:8080/solr" />
-->
<bean id="solrServer" class="org.apache.solr.client.solrj.impl.CloudSolrServer">
<constructor-arg
value="192.168.25.140:2181,192.168.25.140:2182,192.168.25.140:2183" />
<property name="defaultCollection" value="collection1"></property>
</bean>
```

###   分片配置

（1）创建新的 Collection 进行分片处理。
在浏览器输入以下地址，可以按照我们的要求 创建新的 Collection

```url
http://192.168.25.140:8180/solr/admin/collections?action=CREATE&name= collection2 &numShards=2&replicationFactor=2
```

参数：
name :将被创建的集合的名字
numShard s:集合创建时需要创建逻辑碎片的个数
replicationFactor :分片的副本数。
看到这个提示表示成功

![52850811444](H:\itheima大数据项目班\项目班\0609\iamges\1528508114446.png)



![52850812405](H:\itheima大数据项目班\项目班\0609\iamges\1528508124051.png)

（2）删除不用的 Collection。执行以下命令

```html
http://192.168.25.140:8480/solr/admin/collections?action=DELETE&name=collection1
```



![52850816386](H:\itheima大数据项目班\项目班\0609\iamges\1528508163860.png)



### 模拟集群异常测试



（1）停止第一个 tomcat 节点，看查询是否能正常工作 -- 能！因为还有从节点
（2）停止第三个 tomcat 节点，看看查询能够正常工作 -- 不能，因为整个一片数据全没了，无法正常工作。
（3）恢复第三个 tomcat 节点，看看能否正常工作。恢复时间会比较长，大概 2 分半到 3 分钟之间。请耐心等待。

## Redis Cluster

### Redis-Cluster  简介

#### 什么是 Redis-Cluster

为何要搭建 Redis 集群。Redis 是在内存中保存数据的，而我们的电脑一般内存都不大，这也就意味着 Redis 不适合存储大数据，适合存储大数据的是 Hadoop 生态系统的 Hbase 或者是 MogoDB。Redis 更适合处理高并发，一台设备的存储能力是很有限的，但是多台设备协同合作，就可以让内存增大很多倍，这就需要用到集群。

Redis 集群搭建的方式有多种，例如使用客户端分片、Twemproxy、Codis 等，但从redis 3.0 之后版本支持 redis-cluster 集群，它是 Redis 官方提出的解决方案，Redis-Cluster 采用无中心结构，每个节点保存数据和整个集群状态,每个节点都和其他所有节点连接。其 redis-cluster 架构图如下：

![52850822571](H:\itheima大数据项目班\项目班\0609\iamges\1528508225719.png)



客户端与 redis 节点直连,不需要中间 proxy 层.客户端不需要连接集群所有节点连接集群中任何一个可用节点即可。
所有的 redis 节点彼此互联(PING-PONG 机制),内部使用二进制协议优化传输速度和带宽.

#### 分布存储机制-槽

（1）redis-cluster 把所有的物理节点映射到[0-16383]slot 上,cluster 负责维护
node<->slot<->value

2）Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。
例如三个节点：槽分布的值如下：
SERVER1: 0-5460
SERVER2: 5461-10922
SERVER3: 10923-16383

####   容错机制- 投票

（1）选举过程是集群中所有 master 参与,如果半数以上 master 节点与故障节点通信超过(cluster-node-timeout),认为该节点故障，自动触发故障转移操作. 故障节点对应的从节点自动升级为主节点

（2）什么时候整个集群不可用(cluster_state:fail)?
如果集群任意 master 挂掉,且当前 master 没有 slave.集群进入 fail 状态,也可以理解成集群的 slot 映射[0-16383]不完成时进入 fail 状态.

![52850832379](H:\itheima大数据项目班\项目班\0609\iamges\1528508323796.png)

###   搭建 Redis-Cluster

####  搭建要求

需要 6 台 redis 服务器。搭建伪集群。
需要 6 个 redis 实例。
需要运行在不同的端口 7001-7006

#### 准备工作

（1）安装 gcc 【此步省略】
Redis 是 c 语言开发的。安装 redis 需要 c 语言的编译环境。如果没有 gcc 需要在线安装。

```shell
yum install gcc-c++
```

（2）使用 yum 命令安装 ruby （我们需要使用 ruby 脚本来实现集群搭建）【此步省略】

```shell
yum install ruby
yum install rubygems
```



-----  知识点小贴士 -----
Ruby，一种简单快捷的面向对象（面向对象程序设计）脚本语言，在 20 世纪 90 年代由日本人松本行弘Yukihiro Matsumoto)开发，遵守 GPL 协议和 Ruby License。它的灵感与特性来自于 Perl、Smalltalk、Eiffel、
Ada 以及 Lisp 语言。由 Ruby 语言本身还发展出了 JRuby（Java 平台）、IronRuby（.NET 平台）等其他平台的 Ruby 语言替代品。Ruby 的作者于 1993 年 2 月 24 日开始编写 Ruby，直至 1995 年 12 月才正
式公开发布于 fj （新闻组）。因为 Perl 发音与 6 月诞生石 pearl （珍珠）相同，因此 Ruby 以 7 月诞生石 ruby（红宝石）命名RubyGems 简称 gems，是一个用于对 Ruby 组件进行打包的 Ruby 打包系统



（3）将 redis 源码包上传到 linux 系统 ，解压 redis 源码包
（4）编译 redis 源码 ，进入 redis 源码文件夹make

看到以下输出结果，表示编译成功

![52850847271](H:\itheima大数据项目班\项目班\0609\iamges\1528508472717.png)

（5）创建目录/usr/local/redis-cluster 目录， 安装 6 个 redis 实例，分别安装在以下目录

/usr/local/redis-cluster/redis-1
/usr/local/redis-cluster/redis-2
/usr/local/redis-cluster/redis-3
/usr/local/redis-cluster/redis-4
/usr/local/redis-cluster/redis-5
/usr/local/redis-cluster/redis-6

以第一个 redis 实例为例，命令如下

```shell
make install PREFIX=/usr/local/redis-cluster/redis-1
```

![52850851099](H:\itheima大数据项目班\项目班\0609\iamges\1528508510999.png)



（6）复制配置文件 将 /redis-3.0.0/redis.conf 复制到 redis 下的 bin 目录下

```shell
[root@localhost redis-3.0.0]# cp redis.conf /usr/local/redis-cluster/redis-1/bin
[root@localhost redis-3.0.0]# cp redis.conf /usr/local/redis-cluster/redis-2/bin
[root@localhost redis-3.0.0]# cp redis.conf /usr/local/redis-cluster/redis-3/bin
[root@localhost redis-3.0.0]# cp redis.conf /usr/local/redis-cluster/redis-4/bin
[root@localhost redis-3.0.0]# cp redis.conf /usr/local/redis-cluster/redis-5/bin
[root@localhost redis-3.0.0]# cp redis.conf /usr/local/redis-cluster/redis-6/bin
```

#### 配置集群

（1）修改每个 redis 节点的配置文件 redis.conf
修改运行端口为 7001 （7002 7003 .....）

![52850856125](H:\itheima大数据项目班\项目班\0609\iamges\1528508561254.png)

将 cluster-enabled yes 前的注释去掉(632 行)



![52850857640](H:\itheima大数据项目班\项目班\0609\iamges\1528508576406.png)



（2）启动每个 redis 实例

以第一个实例为例，命令如下

```shell
cd /usr/local/redis-cluster/redis-1/bin/
./redis-server redis.conf
```

![52850860536](H:\itheima大数据项目班\项目班\0609\iamges\1528508605365.png)



把其余的 5 个也启动起来，然后查看一下是不是都启动起来了

```shell
[root@localhost ~]# ps -ef | grep redis
root 15776 15775 0 08:19 pts/1 00:00:00 ./redis-server *:7001 [cluster]
root 15810 15784 0 08:22 pts/2 00:00:00 ./redis-server *:7002 [cluster]
root 15831 15813 0 08:23 pts/3 00:00:00 ./redis-server *:7003 [cluster]
root 15852 15834 0 08:23 pts/4 00:00:00 ./redis-server *:7004 [cluster]
root 15872 15856 0 08:24 pts/5 00:00:00 ./redis-server *:7005 [cluster]
root 15891 15875 0 08:24 pts/6 00:00:00 ./redis-server *:7006 [cluster]
root 15926 15895 0 08:24 pts/7 00:00:00 grep redis
```

（3）上传 redis-3.0.0.gem ，安装 ruby 用于搭建 redis 集群的脚本。

```shell
[root@localhost ~]# gem install redis-3.0.0.gem
Successfully installed redis-3.0.0
1 gem installed
Installing ri documentation for redis-3.0.0...
Installing RDoc documentation for redis-3.0.0...
```

（4）使用 ruby 脚本搭建集群。
进入 redis 源码目录中的 src 目录 执行下面的命令

```shell
./redis-trib.rb create --replicas 1 192.168.25.140:7001 192.168.25.140:7002 192.168.25.140:7003 192.168.25.140:7004 192.168.25.140:7005 192.168.25.140:7006
```

出现下列提示信息

```shell
>>> Creating cluster
Connecting to node 192.168.25.140:7001: OK
Connecting to node 192.168.25.140:7002: OK
Connecting to node 192.168.25.140:7003: OK
Connecting to node 192.168.25.140:7004: OK
Connecting to node 192.168.25.140:7005: OK
Connecting to node 192.168.25.140:7006: OK
>>> Performing hash slots allocation on 6 nodes...
Using 3 masters:
192.168.25.140:7001
192.168.25.140:7002
192.168.25.140:7003
Adding replica 192.168.25.140:7004 to 192.168.25.140:7001
Adding replica 192.168.25.140:7005 to 192.168.25.140:7002
Adding replica 192.168.25.140:7006 to 192.168.25.140:7003
M: 1800237a743c2aa918ade045a28128448c6ce689 192.168.25.140:7001
slots:0-5460 (5461 slots) master
M: 7cb3f7d5c60bfbd3ab28800f8fd3bf6de005bf0d 192.168.25.140:7002
slots:5461-10922 (5462 slots) master
M: 436e88ec323a2f8bb08bf09f7df07cc7909fcf81 192.168.25.140:7003
slots:10923-16383 (5461 slots) master
S: c2a39a94b5f41532cd83bf6643e98fc277c2f441 192.168.25.140:7004
replicates 1800237a743c2aa918ade045a28128448c6ce689
S: b0e38d80273515c84b1a01820d8ecee04547d776 192.168.25.140:7005
replicates 7cb3f7d5c60bfbd3ab28800f8fd3bf6de005bf0d
S: 03bf6bd7e3e6eece5a02043224497c2c8e185132 192.168.25.140:7006
replicates 436e88ec323a2f8bb08bf09f7df07cc7909fcf81
Can I set the above configuration? (type 'yes' to accept): yes
>>> Nodes configuration updated
>>> Assign a different config epoch to each node
>>> Sending CLUSTER MEET messages to join the cluster
Waiting for the cluster to join....
>>> Performing Cluster Check (using node 192.168.25.140:7001)
M: 1800237a743c2aa918ade045a28128448c6ce689 192.168.25.140:7001
slots:0-5460 (5461 slots) master
M: 7cb3f7d5c60bfbd3ab28800f8fd3bf6de005bf0d 192.168.25.140:7002
slots:5461-10922 (5462 slots) master
M: 436e88ec323a2f8bb08bf09f7df07cc7909fcf81 192.168.25.140:7003
slots:10923-16383 (5461 slots) master
M: c2a39a94b5f41532cd83bf6643e98fc277c2f441 192.168.25.140:7004
slots: (0 slots) master
replicates 1800237a743c2aa918ade045a28128448c6ce689
M: b0e38d80273515c84b1a01820d8ecee04547d776 192.168.25.140:7005
slots: (0 slots) master
replicates 7cb3f7d5c60bfbd3ab28800f8fd3bf6de005bf0d
M: 03bf6bd7e3e6eece5a02043224497c2c8e185132 192.168.25.140:7006
slots: (0 slots) master
replicates 436e88ec323a2f8bb08bf09f7df07cc7909fcf81
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
```

### 连接 Redis-Cluster

#### 客户端工具连接

Redis-cli 连接集群：

```she
redis-cli -p 主机 ip -p 端口（集群中任意端口） -c
```

-c：代表连接的是 redis 集群

测试值的存取:
（1）从本地连接到集群 redis 使用 7001 端口 加 -c 参数
（2）存入 name 值为 abc ，系统提示此值被存入到了 7002 端口所在的 redis （槽是 5798）
（3）提取 name 的值，可以提取。
（4）退出（quit）
（5）再次以 7001 端口进入 ，不带-c
（6）查询 name 值，无法获取，因为值在 7002 端口的 redis 上
（7）我们以 7002 端口进入，获取 name 值发现是可以获取的,而以其它端口进入均不能获取

####  SpringDataRedis  连接 Redis 

修改品优购工程 在 pinyougou-common 工程添加 spring 配置文件
applicationContext-redis-cluster.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xmlns:p="http://www.springframework.org/schema/p"
xmlns:context="http://www.springframework.org/schema/context"
xsi:schemaLocation="http://www.springframework.org/schema/beans
http://www.springframework.org/schema/beans/spring-beans.xsd
http://www.springframework.org/schema/context
http://www.springframework.org/schema/context/spring-context.xsd">
<!-- 加载配置属性文件 -->
<context:property-placeholder ignore-unresolvable="true"
location="classpath:properties/redis-cluster-config.properties" />
<bean id="redis-clusterConfiguration"
class="org.springframework.data.redis.connection.redis-clusterConfiguration">
<property name="maxRedirects" value="${redis.maxRedirects}"></property>
<property name="clusterNodes">
<set>
<bean class="org.springframework.data.redis.connection.redis-clusterNode">
<constructor-arg name="host" value="${redis.host1}"></constructor-arg>
<constructor-arg name="port" value="${redis.port1}"></constructor-arg>
</bean>
<bean class="org.springframework.data.redis.connection.redis-clusterNode">
<constructor-arg name="host" value="${redis.host2}"></constructor-arg>
<constructor-arg name="port" value="${redis.port2}"></constructor-arg>
</bean>
<bean class="org.springframework.data.redis.connection.redis-clusterNode">
<constructor-arg name="host" value="${redis.host3}"></constructor-arg>
<constructor-arg name="port" value="${redis.port3}"></constructor-arg>
</bean>
 <bean class="org.springframework.data.redis.connection.redis-clusterNode">
<constructor-arg name="host" value="${redis.host4}"></constructor-arg>
<constructor-arg name="port" value="${redis.port4}"></constructor-arg>
</bean>
<bean class="org.springframework.data.redis.connection.redis-clusterNode">
<constructor-arg name="host" value="${redis.host5}"></constructor-arg>
<constructor-arg name="port" value="${redis.port5}"></constructor-arg>
</bean>
<bean class="org.springframework.data.redis.connection.redis-clusterNode">
<constructor-arg name="host" value="${redis.host6}"></constructor-arg>
<constructor-arg name="port" value="${redis.port6}"></constructor-arg>
</bean>
</set>
</property>
</bean>
<bean id="jedisPoolConfig" class="redis.clients.jedis.JedisPoolConfig">
<property name="maxIdle" value="${redis.maxIdle}" />
<property name="maxTotal" value="${redis.maxTotal}" />
</bean>
<bean id="jeidsConnectionFactory"
class="org.springframework.data.redis.connection.jedis.JedisConnectionFactory" >
<constructor-arg ref="redis-clusterConfiguration" />
<constructor-arg ref="jedisPoolConfig" />
</bean>   
<bean id="redisTemplate" class="org.springframework.data.redis.core.RedisTemplate">
<property name="connectionFactory" ref="jeidsConnectionFactory" />
</bean>
</beans>    
```

添加属性文件 redis-cluster-config.properties

```xml
#cluster configuration
redis.host1=192.168.25.140
redis.port1=7001
redis.host2=192.168.25.140
redis.port2=7002
redis.host3=192.168.25.140
redis.port3=7003
redis.host4=192.168.25.140
redis.port4=7004
redis.host5=192.168.25.140
redis.port5=7005
redis.host6=192.168.25.140
redis.port6=7006
redis.maxRedirects=3
redis.maxIdle=100
redis.maxTotal=600
```

### 模拟集群异常测试

关闭节点命令

```shell
./redis-cli -p 端口 shutdown
```

（1）测试关闭 7001 和 7004, 看看会发生什么。
（2）测试关闭 7001、7002、7003 会发生什么。

